Clustering, Dimensionality Reduction, and Expectation-Maximization

This repository contains the solutions to the fourth practical assignment for the course ‚ÄúI302 - Machine Learning and Deep Learning‚Äù for the first semester of 2024.

üìö Table of Contents
	‚Ä¢	Introduction
	‚Ä¢	Problem Descriptions
	‚Ä¢	1. Data Clustering
	‚Ä¢	2. Dimensionality Reduction
	‚Ä¢	3. Expectation-Maximization
	‚Ä¢	Results
	‚Ä¢	Clustering Results
	‚Ä¢	PCA Results
	‚Ä¢	Expectation-Maximization Results

üìù Introduction

This repository contains a collection of Jupyter Notebooks and Python scripts developed for the fourth practical assignment of the ‚ÄúI302 - Machine Learning and Deep Learning‚Äù course. The assignment focuses on three key topics: Data Clustering, Dimensionality Reduction, and Expectation-Maximization (EM).

The problems were solved using basic tools such as NumPy, Pandas, Matplotlib, and optionally PyTorch for more advanced tasks. Each problem is organized into separate folders, with corresponding notebooks and auxiliary scripts when necessary.

üìä Problem Descriptions

1. Data Clustering

The goal of this problem is to analyze the dataset clustering.csv using various clustering techniques:
	‚Ä¢	K-Means Algorithm:
	‚Ä¢	Implemented to cluster data points.
	‚Ä¢	The optimal number of clusters was determined using the ‚Äúelbow method‚Äù.
	‚Ä¢	Gaussian Mixture Model (GMM):
	‚Ä¢	Implemented to fit a probabilistic model to the data.
	‚Ä¢	Initialization was performed using K-Means.
	‚Ä¢	DBSCAN Algorithm:
	‚Ä¢	Explored the effect of varying parameters such as œµ (neighborhood radius) and K (minimum points per dense region).
	‚Ä¢	Optimal parameters were selected, and clusters were visualized.

The implementation can be found in:
	‚Ä¢	ML-TP2/
	‚Ä¢	clustering_analysis.ipynb
	‚Ä¢	clustering_functions.py

2. Dimensionality Reduction

This problem uses the MNIST dataset.csv to reduce the dimensionality of image data:
	‚Ä¢	Principal Component Analysis (PCA):
	‚Ä¢	Implemented to reduce data dimensionality.
	‚Ä¢	The reconstruction error was analyzed across varying numbers of principal components.
	‚Ä¢	Image Reconstruction:
	‚Ä¢	Original and reconstructed images were compared visually.
	‚Ä¢	(Optional) Variational Autoencoder (VAE):
	‚Ä¢	Built using PyTorch to compare its performance with PCA.

The implementation can be found in:
	‚Ä¢	ML-TP3/
	‚Ä¢	pca_analysis.ipynb
	‚Ä¢	pca_functions.py

3. Expectation-Maximization

This problem focuses on deriving and implementing the Expectation-Maximization (EM) algorithm for Gaussian Mixture Models (GMM):
	‚Ä¢	E-step:
	‚Ä¢	Derivation of the Q(w, w‚ÇÄ) function.
	‚Ä¢	M-step:
	‚Ä¢	Optimization of parameters ¬µ, Œ£, and œÄ.
	‚Ä¢	Mathematical Demonstration:
	‚Ä¢	Proof that the Q(w, w‚ÇÄ) function is a lower bound on the log-likelihood.

The implementation can be found in:
	‚Ä¢	ML-TP4/
	‚Ä¢	em_algorithm.ipynb
	‚Ä¢	(Mathematical derivations are included as an image inside the notebook.)

üìà Results

Clustering Results

Visualization of the clustering results for K-Means, GMM, and DBSCAN. Each plot shows the data distribution, assigned clusters, and centroids (if applicable).

PCA Results

Analysis of the reconstruction error as a function of the number of principal components, along with side-by-side visual comparisons of original and reconstructed images.

Expectation-Maximization Results

Mathematical derivations, optimized parameter values, and graphical representation of the Gaussian Mixture Model (GMM) results.

üöÄ How to Run the Notebooks
	1.	Clone the repository:

git clone https://github.com/your-username/ML-TP4.git
cd ML-TP4


	2.	Install the necessary dependencies:

pip install -r requirements.txt


	3.	Launch Jupyter Notebook:

jupyter notebook


	4.	Open the corresponding .ipynb files from each folder.

ü§ù Contributions

This practical assignment was developed by [M√°ximo Gubitosi] as part of the ‚ÄúI302 - Machine Learning and Deep Learning‚Äù course during the last semester of 2023.